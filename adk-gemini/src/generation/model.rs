use reqwest::Url;
use serde::{Deserialize, Serialize};
use snafu::Snafu;
use time::OffsetDateTime;

use crate::{
    Content, Modality, Part,
    safety::{SafetyRating, SafetySetting},
};

hybrid_enum! {
    /// Reason why generation finished
    pub enum FinishReason {
        /// Default value. This value is unused.
        FinishReasonUnspecified => ("FINISH_REASON_UNSPECIFIED", 0),
        /// Natural stop point of the model or provided stop sequence.
        Stop                    => ("STOP", 1),
        /// The maximum number of tokens as specified in the request was reached.
        MaxTokens               => ("MAX_TOKENS", 2),
        /// The response candidate content was flagged for safety reasons.
        Safety                  => ("SAFETY", 3),
        /// The response candidate content was flagged for recitation reasons.
        Recitation              => ("RECITATION", 4),
        /// Unknown reason.
        Other                   => ("OTHER", 5),
        /// Token generation stopped because the content contains forbidden terms.
        Blocklist               => ("BLOCKLIST", 6),
        /// Token generation stopped for potentially containing prohibited content.
        ProhibitedContent       => ("PROHIBITED_CONTENT", 7),
        /// Content potentially contains Sensitive Personally Identifiable Information (SPII).
        Spii                    => ("SPII", 8),
        /// The function call generated by the model is invalid.
        MalformedFunctionCall   => ("MALFORMED_FUNCTION_CALL", 9),
        /// Token generation stopped because generated images contain safety violations.
        ImageSafety             => ("IMAGE_SAFETY", 10),
        /// Model generated a tool call but no tools were enabled in the request.
        UnexpectedToolCall      => ("UNEXPECTED_TOOL_CALL", 11),
        /// The response candidate content was flagged for using an unsupported language.
        Language                => ("LANGUAGE", 12),
        /// Model called too many tools consecutively.
        TooManyToolCalls        => ("TOO_MANY_TOOL_CALLS", 13),
    }
    fallback: Other
}

/// Citation metadata for content
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "camelCase")]
pub struct CitationMetadata {
    /// The citation sources
    pub citation_sources: Vec<CitationSource>,
}

/// Citation source
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "camelCase")]
pub struct CitationSource {
    /// The URI of the citation source
    pub uri: Option<String>,
    /// The title of the citation source
    pub title: Option<String>,
    /// The start index of the citation in the response
    pub start_index: Option<i32>,
    /// The end index of the citation in the response
    pub end_index: Option<i32>,
    /// The license of the citation source
    pub license: Option<String>,
    /// The publication date of the citation source
    #[serde(default, with = "time::serde::rfc3339::option")]
    pub publication_date: Option<OffsetDateTime>,
}

/// A candidate response
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "camelCase")]
pub struct Candidate {
    /// The content of the candidate
    #[serde(default)]
    pub content: Content,
    /// The safety ratings for the candidate
    #[serde(skip_serializing_if = "Option::is_none")]
    pub safety_ratings: Option<Vec<SafetyRating>>,
    /// The citation metadata for the candidate
    #[serde(skip_serializing_if = "Option::is_none")]
    pub citation_metadata: Option<CitationMetadata>,
    /// The grounding metadata for the candidate
    #[serde(skip_serializing_if = "Option::is_none")]
    pub grounding_metadata: Option<GroundingMetadata>,
    /// The finish reason for the candidate
    #[serde(skip_serializing_if = "Option::is_none")]
    pub finish_reason: Option<FinishReason>,
    /// The index of the candidate
    #[serde(skip_serializing_if = "Option::is_none")]
    pub index: Option<i32>,
}

/// Metadata about token usage
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "camelCase")]
pub struct UsageMetadata {
    /// The number of prompt tokens (null if request processing failed)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub prompt_token_count: Option<i32>,
    /// The number of response tokens (null if generation failed)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub candidates_token_count: Option<i32>,
    /// The total number of tokens (null if individual counts unavailable)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub total_token_count: Option<i32>,
    /// The number of thinking tokens (Gemini 2.5 series only)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub thoughts_token_count: Option<i32>,
    /// Detailed prompt token information
    #[serde(skip_serializing_if = "Option::is_none")]
    pub prompt_tokens_details: Option<Vec<PromptTokenDetails>>,
    /// The number of cached content tokens (batch API)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub cached_content_token_count: Option<i32>,
    /// Detailed cache token information (batch API)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub cache_tokens_details: Option<Vec<PromptTokenDetails>>,
}

/// Details about prompt tokens by modality
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "camelCase")]
pub struct PromptTokenDetails {
    /// The modality (e.g., "TEXT")
    pub modality: Modality,
    /// Token count for this modality
    pub token_count: i32,
}

/// Grounding metadata for responses that use grounding tools
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "camelCase")]
pub struct GroundingMetadata {
    /// Grounding chunks containing source information
    #[serde(skip_serializing_if = "Option::is_none")]
    pub grounding_chunks: Option<Vec<GroundingChunk>>,
    /// Grounding supports connecting response text to sources
    #[serde(skip_serializing_if = "Option::is_none")]
    pub grounding_supports: Option<Vec<GroundingSupport>>,
    /// Web search queries used for grounding
    #[serde(skip_serializing_if = "Option::is_none")]
    pub web_search_queries: Option<Vec<String>>,
    /// Google Maps widget context token
    #[serde(skip_serializing_if = "Option::is_none")]
    pub google_maps_widget_context_token: Option<String>,
}

/// A chunk of grounding information from a source
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "camelCase")]
pub struct GroundingChunk {
    /// Maps-specific grounding information
    #[serde(skip_serializing_if = "Option::is_none")]
    pub maps: Option<MapsGroundingChunk>,
    /// Web-specific grounding information
    #[serde(skip_serializing_if = "Option::is_none")]
    pub web: Option<WebGroundingChunk>,
}

/// Maps-specific grounding chunk information
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "camelCase")]
pub struct MapsGroundingChunk {
    /// The URI of the Maps source
    #[serde(default)]
    pub uri: Option<Url>,
    /// The title of the Maps source
    #[serde(default)]
    pub title: Option<String>,
    /// The place ID from Google Maps
    #[serde(skip_serializing_if = "Option::is_none")]
    pub place_id: Option<String>,
}

/// Web-specific grounding chunk information
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "camelCase")]
pub struct WebGroundingChunk {
    /// The URI of the web source
    #[serde(default)]
    pub uri: Option<Url>,
    /// The title of the web source
    #[serde(default)]
    pub title: Option<String>,
}

/// Support information connecting response text to grounding sources
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "camelCase")]
pub struct GroundingSupport {
    /// Segment of the response text
    pub segment: GroundingSegment,
    /// Indices of grounding chunks that support this segment
    pub grounding_chunk_indices: Vec<u32>,
}

/// A segment of response text
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq, Default)]
#[serde(rename_all = "camelCase")]
pub struct GroundingSegment {
    /// Start index of the segment in the response text
    #[serde(default)]
    pub start_index: Option<u32>,
    /// End index of the segment in the response text
    #[serde(default)]
    pub end_index: Option<u32>,
    /// The text content of the segment
    #[serde(default)]
    pub text: Option<String>,
}

/// Response from the Gemini API for content generation
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "camelCase")]
pub struct GenerationResponse {
    /// The candidates generated
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub candidates: Vec<Candidate>,
    /// The prompt feedback
    #[serde(skip_serializing_if = "Option::is_none")]
    pub prompt_feedback: Option<PromptFeedback>,
    /// Usage metadata
    #[serde(skip_serializing_if = "Option::is_none")]
    pub usage_metadata: Option<UsageMetadata>,
    /// Model version used
    #[serde(skip_serializing_if = "Option::is_none")]
    pub model_version: Option<String>,
    /// Response ID
    #[serde(skip_serializing_if = "Option::is_none")]
    pub response_id: Option<String>,
}

hybrid_enum! {
    /// Reason why content was blocked
    pub enum BlockReason {
        /// Default value. This value is unused.
        BlockReasonUnspecified => ("BLOCK_REASON_UNSPECIFIED", 0),
        /// Prompt was blocked due to safety reasons.
        Safety                => ("SAFETY", 1),
        /// Prompt was blocked due to unknown reasons.
        Other                 => ("OTHER", 2),
        /// Prompt was blocked due to the terms which are included from the terminology blocklist.
        Blocklist             => ("BLOCKLIST", 3),
        /// Prompt was blocked due to prohibited content.
        ProhibitedContent     => ("PROHIBITED_CONTENT", 4),
        /// Candidates blocked due to unsafe image generation content.
        ImageSafety           => ("IMAGE_SAFETY", 5),
        /// Candidates blocked due to model armor.
        ModelArmor            => ("MODEL_ARMOR", 6),
    }
    fallback: Other
}

/// Feedback about the prompt
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "camelCase")]
pub struct PromptFeedback {
    /// The safety ratings for the prompt
    #[serde(default, skip_serializing_if = "Vec::is_empty")]
    pub safety_ratings: Vec<SafetyRating>,
    /// The block reason if the prompt was blocked
    #[serde(skip_serializing_if = "Option::is_none")]
    pub block_reason: Option<BlockReason>,
}

impl GenerationResponse {
    /// Get the text of the first candidate
    pub fn text(&self) -> String {
        self.candidates
            .first()
            .and_then(|c| {
                c.content.parts.as_ref().and_then(|parts| {
                    parts.first().and_then(|p| match p {
                        Part::Text { text, thought: _, thought_signature: _ } => Some(text.clone()),
                        _ => None,
                    })
                })
            })
            .unwrap_or_default()
    }

    /// Get function calls from the response
    pub fn function_calls(&self) -> Vec<&crate::tools::FunctionCall> {
        self.candidates
            .iter()
            .flat_map(|c| {
                c.content
                    .parts
                    .as_ref()
                    .map(|parts| {
                        parts
                            .iter()
                            .filter_map(|p| match p {
                                Part::FunctionCall { function_call, thought_signature: _ } => {
                                    Some(function_call)
                                }
                                _ => None,
                            })
                            .collect::<Vec<_>>()
                    })
                    .unwrap_or_default()
            })
            .collect()
    }

    /// Get function calls with their thought signatures from the response
    pub fn function_calls_with_thoughts(
        &self,
    ) -> Vec<(&crate::tools::FunctionCall, Option<&String>)> {
        self.candidates
            .iter()
            .flat_map(|c| {
                c.content
                    .parts
                    .as_ref()
                    .map(|parts| {
                        parts
                            .iter()
                            .filter_map(|p| match p {
                                Part::FunctionCall { function_call, thought_signature } => {
                                    Some((function_call, thought_signature.as_ref()))
                                }
                                _ => None,
                            })
                            .collect::<Vec<_>>()
                    })
                    .unwrap_or_default()
            })
            .collect()
    }

    /// Get thought summaries from the response
    pub fn thoughts(&self) -> Vec<String> {
        self.candidates
            .iter()
            .flat_map(|c| {
                c.content
                    .parts
                    .as_ref()
                    .map(|parts| {
                        parts
                            .iter()
                            .filter_map(|p| match p {
                                Part::Text { text, thought: Some(true), thought_signature: _ } => {
                                    Some(text.clone())
                                }
                                _ => None,
                            })
                            .collect::<Vec<_>>()
                    })
                    .unwrap_or_default()
            })
            .collect()
    }

    /// Get all text parts (both regular text and thoughts)
    pub fn all_text(&self) -> Vec<(String, bool)> {
        self.candidates
            .iter()
            .flat_map(|c| {
                c.content
                    .parts
                    .as_ref()
                    .map(|parts| {
                        parts
                            .iter()
                            .filter_map(|p| match p {
                                Part::Text { text, thought, thought_signature: _ } => {
                                    Some((text.clone(), thought.unwrap_or(false)))
                                }
                                _ => None,
                            })
                            .collect::<Vec<_>>()
                    })
                    .unwrap_or_default()
            })
            .collect()
    }

    /// Get text parts with their thought signatures from the response
    pub fn text_with_thoughts(&self) -> Vec<(String, bool, Option<&String>)> {
        self.candidates
            .iter()
            .flat_map(|c| {
                c.content
                    .parts
                    .as_ref()
                    .map(|parts| {
                        parts
                            .iter()
                            .filter_map(|p| match p {
                                Part::Text { text, thought, thought_signature } => Some((
                                    text.clone(),
                                    thought.unwrap_or(false),
                                    thought_signature.as_ref(),
                                )),
                                _ => None,
                            })
                            .collect::<Vec<_>>()
                    })
                    .unwrap_or_default()
            })
            .collect()
    }
}

#[derive(Debug, Snafu, PartialEq)]
#[snafu(visibility(pub))]
pub enum ValidationError {
    #[snafu(display("temperature must be between 0.0 and 1.0, got {value}"))]
    InvalidTemperature { value: f32 },
    #[snafu(display("top_p must be between 0.0 and 1.0, got {value}"))]
    InvalidTopP { value: f32 },
    #[snafu(display("top_k must be greater than 0, got {value}"))]
    InvalidTopK { value: i32 },
    #[snafu(display("max_output_tokens must be greater than 0, got {value}"))]
    InvalidMaxOutputTokens { value: i32 },
    #[snafu(display("candidate_count must be greater than 0, got {value}"))]
    InvalidCandidateCount { value: i32 },
    #[snafu(display("thinking_budget must be -1 or greater, got {value}"))]
    InvalidThinkingBudget { value: i32 },
}

/// Request to generate content
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GenerateContentRequest {
    /// The contents to generate content from
    pub contents: Vec<Content>,
    /// The generation config
    #[serde(skip_serializing_if = "Option::is_none")]
    pub generation_config: Option<GenerationConfig>,
    /// The safety settings
    #[serde(skip_serializing_if = "Option::is_none")]
    pub safety_settings: Option<Vec<SafetySetting>>,
    /// The tools that the model can use
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tools: Option<Vec<crate::tools::Tool>>,
    /// The tool config
    #[serde(skip_serializing_if = "Option::is_none")]
    pub tool_config: Option<crate::tools::ToolConfig>,
    /// The system instruction
    #[serde(skip_serializing_if = "Option::is_none")]
    pub system_instruction: Option<Content>,
    /// The cached content to use
    #[serde(skip_serializing_if = "Option::is_none")]
    pub cached_content: Option<String>,
}

/// Configuration for thinking (Gemini 2.5 series only)
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct ThinkingConfig {
    /// The thinking budget (number of thinking tokens)
    ///
    /// - Set to 0 to disable thinking
    /// - Set to -1 for dynamic thinking (model decides)
    /// - Set to a positive number for a specific token budget
    ///
    /// Model-specific ranges:
    /// - 2.5 Pro: 128 to 32768 (cannot disable thinking)
    /// - 2.5 Flash: 0 to 24576
    /// - 2.5 Flash Lite: 512 to 24576
    #[serde(skip_serializing_if = "Option::is_none")]
    pub thinking_budget: Option<i32>,

    /// Whether to include thought summaries in the response
    ///
    /// When enabled, the response will include synthesized versions of the model's
    /// raw thoughts, providing insights into the reasoning process.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub include_thoughts: Option<bool>,
}

impl ThinkingConfig {
    /// Create a new thinking config and validate its parameters
    pub fn try_new(
        thinking_budget: Option<i32>,
        include_thoughts: Option<bool>,
    ) -> Result<Self, ValidationError> {
        let config = Self { thinking_budget, include_thoughts };
        config.validate()?;
        Ok(config)
    }

    /// Validate the thinking configuration parameters
    pub fn validate(&self) -> Result<(), ValidationError> {
        if let Some(budget) = self.thinking_budget
            && budget < -1
        {
            return Err(ValidationError::InvalidThinkingBudget { value: budget });
        }
        Ok(())
    }

    /// Create a new thinking config with default settings
    pub fn new() -> Self {
        Self { thinking_budget: None, include_thoughts: None }
    }

    /// Set the thinking budget
    pub fn with_thinking_budget(mut self, budget: i32) -> Self {
        self.thinking_budget = Some(budget);
        self
    }

    /// Enable dynamic thinking (model decides the budget)
    pub fn with_dynamic_thinking(mut self) -> Self {
        self.thinking_budget = Some(-1);
        self
    }

    /// Include thought summaries in the response
    pub fn with_thoughts_included(mut self, include: bool) -> Self {
        self.include_thoughts = Some(include);
        self
    }

    /// Create a thinking config that enables dynamic thinking with thoughts included
    pub fn dynamic_thinking() -> Self {
        Self { thinking_budget: Some(-1), include_thoughts: Some(true) }
    }
}

impl Default for ThinkingConfig {
    fn default() -> Self {
        Self::new()
    }
}

/// Configuration for generation
#[derive(Debug, Default, Clone, Serialize, Deserialize)]
#[serde(rename_all = "camelCase")]
pub struct GenerationConfig {
    /// The temperature for the model (0.0 to 1.0)
    ///
    /// Controls the randomness of the output. Higher values (e.g., 0.9) make output
    /// more random, lower values (e.g., 0.1) make output more deterministic.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub temperature: Option<f32>,

    /// The top-p value for the model (0.0 to 1.0)
    ///
    /// For each token generation step, the model considers the top_p percentage of
    /// probability mass for potential token choices. Lower values are more selective,
    /// higher values allow more variety.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_p: Option<f32>,

    /// The top-k value for the model
    ///
    /// For each token generation step, the model considers the top_k most likely tokens.
    /// Lower values are more selective, higher values allow more variety.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub top_k: Option<i32>,

    /// The maximum number of tokens to generate
    ///
    /// Limits the length of the generated content. One token is roughly 4 characters.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub max_output_tokens: Option<i32>,

    /// The candidate count
    ///
    /// Number of alternative responses to generate.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub candidate_count: Option<i32>,

    /// Whether to stop on specific sequences
    ///
    /// The model will stop generating content when it encounters any of these sequences.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub stop_sequences: Option<Vec<String>>,

    /// The response mime type
    ///
    /// Specifies the format of the model's response.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub response_mime_type: Option<String>,
    /// The response schema
    ///
    /// Specifies the JSON schema for structured responses.
    #[serde(skip_serializing_if = "Option::is_none")]
    pub response_schema: Option<serde_json::Value>,

    /// Response modalities (for TTS and other multimodal outputs)
    #[serde(skip_serializing_if = "Option::is_none")]
    pub response_modalities: Option<Vec<String>>,

    /// Speech configuration for text-to-speech generation
    #[serde(skip_serializing_if = "Option::is_none")]
    pub speech_config: Option<SpeechConfig>,

    /// The thinking configuration
    ///
    /// Configuration for the model's thinking process (Gemini 2.5 series only).
    #[serde(skip_serializing_if = "Option::is_none")]
    pub thinking_config: Option<ThinkingConfig>,
}

impl GenerationConfig {
    /// Create a new generation config and validate its parameters
    #[allow(clippy::too_many_arguments)]
    pub fn try_new(
        temperature: Option<f32>,
        top_p: Option<f32>,
        top_k: Option<i32>,
        max_output_tokens: Option<i32>,
        candidate_count: Option<i32>,
        stop_sequences: Option<Vec<String>>,
        response_mime_type: Option<String>,
        response_schema: Option<serde_json::Value>,
        response_modalities: Option<Vec<String>>,
        speech_config: Option<SpeechConfig>,
        thinking_config: Option<ThinkingConfig>,
    ) -> Result<Self, ValidationError> {
        let config = Self {
            temperature,
            top_p,
            top_k,
            max_output_tokens,
            candidate_count,
            stop_sequences,
            response_mime_type,
            response_schema,
            response_modalities,
            speech_config,
            thinking_config,
        };
        config.validate()?;
        Ok(config)
    }

    /// Validate the generation configuration parameters
    pub fn validate(&self) -> Result<(), ValidationError> {
        if let Some(t) = self.temperature
            && !(0.0..=1.0).contains(&t)
        {
            return Err(ValidationError::InvalidTemperature { value: t });
        }
        if let Some(p) = self.top_p
            && !(0.0..=1.0).contains(&p)
        {
            return Err(ValidationError::InvalidTopP { value: p });
        }
        if let Some(k) = self.top_k
            && k <= 0
        {
            return Err(ValidationError::InvalidTopK { value: k });
        }
        if let Some(m) = self.max_output_tokens
            && m <= 0
        {
            return Err(ValidationError::InvalidMaxOutputTokens { value: m });
        }
        if let Some(c) = self.candidate_count
            && c <= 0
        {
            return Err(ValidationError::InvalidCandidateCount { value: c });
        }
        if let Some(thinking) = &self.thinking_config {
            thinking.validate()?;
        }
        Ok(())
    }
}

impl GenerateContentRequest {
    /// Validate the request parameters
    pub fn validate(&self) -> Result<(), ValidationError> {
        if let Some(config) = &self.generation_config {
            config.validate()?;
        }
        Ok(())
    }
}

/// Configuration for speech generation (text-to-speech)
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "camelCase")]
pub struct SpeechConfig {
    /// Single voice configuration
    #[serde(skip_serializing_if = "Option::is_none")]
    pub voice_config: Option<VoiceConfig>,
    /// Multi-speaker voice configuration
    #[serde(skip_serializing_if = "Option::is_none")]
    pub multi_speaker_voice_config: Option<MultiSpeakerVoiceConfig>,
}

/// Voice configuration for text-to-speech
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "camelCase")]
pub struct VoiceConfig {
    /// Prebuilt voice configuration
    #[serde(skip_serializing_if = "Option::is_none")]
    pub prebuilt_voice_config: Option<PrebuiltVoiceConfig>,
}

/// Prebuilt voice configuration
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "camelCase")]
pub struct PrebuiltVoiceConfig {
    /// The name of the voice to use
    pub voice_name: String,
}

/// Multi-speaker voice configuration
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "camelCase")]
pub struct MultiSpeakerVoiceConfig {
    /// Configuration for each speaker
    pub speaker_voice_configs: Vec<SpeakerVoiceConfig>,
}

/// Configuration for a specific speaker in multi-speaker TTS
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
#[serde(rename_all = "camelCase")]
pub struct SpeakerVoiceConfig {
    /// The name of the speaker (must match the name used in the prompt)
    pub speaker: String,
    /// Voice configuration for this speaker
    pub voice_config: VoiceConfig,
}

impl SpeechConfig {
    /// Create a new speech config with a single voice
    pub fn single_voice(voice_name: impl Into<String>) -> Self {
        Self {
            voice_config: Some(VoiceConfig {
                prebuilt_voice_config: Some(PrebuiltVoiceConfig { voice_name: voice_name.into() }),
            }),
            multi_speaker_voice_config: None,
        }
    }

    /// Create a new speech config with multiple speakers
    pub fn multi_speaker(speakers: Vec<SpeakerVoiceConfig>) -> Self {
        Self {
            voice_config: None,
            multi_speaker_voice_config: Some(MultiSpeakerVoiceConfig {
                speaker_voice_configs: speakers,
            }),
        }
    }
}

impl SpeakerVoiceConfig {
    /// Create a new speaker voice configuration
    pub fn new(speaker: impl Into<String>, voice_name: impl Into<String>) -> Self {
        Self {
            speaker: speaker.into(),
            voice_config: VoiceConfig {
                prebuilt_voice_config: Some(PrebuiltVoiceConfig { voice_name: voice_name.into() }),
            },
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_thinking_config_validation() {
        assert!(ThinkingConfig::try_new(Some(-1), Some(true)).is_ok());
        assert!(ThinkingConfig::try_new(Some(0), Some(false)).is_ok());
        assert!(ThinkingConfig::try_new(Some(100), None).is_ok());

        let err = ThinkingConfig::try_new(Some(-2), None).unwrap_err();
        assert_eq!(err, ValidationError::InvalidThinkingBudget { value: -2 });
    }

    #[test]
    fn test_generation_config_validation() {
        assert!(
            GenerationConfig::try_new(
                Some(0.5),
                Some(0.5),
                Some(40),
                Some(100),
                Some(1),
                None,
                None,
                None,
                None,
                None,
                None
            )
            .is_ok()
        );

        // Invalid temperature
        let err = GenerationConfig::try_new(
            Some(1.5),
            None,
            None,
            None,
            None,
            None,
            None,
            None,
            None,
            None,
            None,
        )
        .unwrap_err();
        assert_eq!(err, ValidationError::InvalidTemperature { value: 1.5 });

        // Invalid top_p
        let err = GenerationConfig::try_new(
            None,
            Some(-0.1),
            None,
            None,
            None,
            None,
            None,
            None,
            None,
            None,
            None,
        )
        .unwrap_err();
        assert_eq!(err, ValidationError::InvalidTopP { value: -0.1 });

        // Invalid top_k
        let err = GenerationConfig::try_new(
            None,
            None,
            Some(0),
            None,
            None,
            None,
            None,
            None,
            None,
            None,
            None,
        )
        .unwrap_err();
        assert_eq!(err, ValidationError::InvalidTopK { value: 0 });

        // Invalid max_output_tokens
        let err = GenerationConfig::try_new(
            None,
            None,
            None,
            Some(0),
            None,
            None,
            None,
            None,
            None,
            None,
            None,
        )
        .unwrap_err();
        assert_eq!(err, ValidationError::InvalidMaxOutputTokens { value: 0 });

        // Invalid candidate_count
        let err = GenerationConfig::try_new(
            None,
            None,
            None,
            None,
            Some(-1),
            None,
            None,
            None,
            None,
            None,
            None,
        )
        .unwrap_err();
        assert_eq!(err, ValidationError::InvalidCandidateCount { value: -1 });

        // Nested validation (thinking_config)
        let thinking_config = ThinkingConfig { thinking_budget: Some(-5), include_thoughts: None };
        let err = GenerationConfig::try_new(
            None,
            None,
            None,
            None,
            None,
            None,
            None,
            None,
            None,
            None,
            Some(thinking_config),
        )
        .unwrap_err();
        assert_eq!(err, ValidationError::InvalidThinkingBudget { value: -5 });
    }

    #[tokio::test]
    async fn test_builder_validation_integration() {
        use crate::Gemini;
        let client = Gemini::new_with_api_key("api-key");

        // Use with_temperature with invalid value
        let builder = client.generate_content().with_temperature(2.0);

        let result = builder.execute().await;
        assert!(result.is_err());

        // Check if it's a validation error
        if let Err(crate::error::Error::Validation { source }) = result {
            let err = source.downcast_ref::<ValidationError>().expect("Not a ValidationError");
            assert_eq!(err, &ValidationError::InvalidTemperature { value: 2.0 });
        } else {
            panic!("Expected validation error, got {:?}", result);
        }
    }

    #[tokio::test]
    async fn test_builder_thinking_validation_integration() {
        use crate::Gemini;
        let client = Gemini::new_with_api_key("api-key");

        // Use with_thinking_budget with invalid value
        let builder = client.generate_content().with_thinking_budget(-2);

        let result = builder.execute().await;
        assert!(result.is_err());

        // Check if it's a validation error
        if let Err(crate::error::Error::Validation { source }) = result {
            let err = source.downcast_ref::<ValidationError>().expect("Not a ValidationError");
            assert_eq!(err, &ValidationError::InvalidThinkingBudget { value: -2 });
        } else {
            panic!("Expected validation error, got {:?}", result);
        }
    }
}
