# =============================================================================
# Ralph Multi-Agent Autonomous Development System - TOML Configuration
# =============================================================================
# Copy this file to ralph.toml in your project directory.
# Environment variables take precedence over this file.
#
# This file provides an alternative to .env for configuration.
# TOML format allows for structured, hierarchical configuration.
# =============================================================================

# =============================================================================
# Execution Settings
# =============================================================================
[execution]
# Maximum number of loop iterations before terminating
# Range: 1-1000, Default: 50
max_iterations = 50

# Maximum retries for a failed task before marking it as blocked
# Range: 1-10, Default: 3
max_task_retries = 3

# Message displayed when all tasks are completed
completion_promise = "All tasks completed successfully!"

# =============================================================================
# File Paths
# =============================================================================
[paths]
# Base directory for the project (where files are created)
# All other paths are relative to this
project = "."

# Path to the Product Requirements Document
prd = "prd.md"

# Path to the design document
design = "design.md"

# Path to the tasks file (JSON format)
tasks = "tasks.json"

# Path to the progress log file (JSON format, append-only)
progress = "progress.json"

# =============================================================================
# PRD Agent Configuration
# =============================================================================
# Creates structured requirements from user prompts
# Recommended: Use thinking-enabled model for deep analysis
[agents.prd]
# Model provider: anthropic, openai, gemini, ollama
provider = "anthropic"

# Model name (provider-specific)
# Anthropic: claude-opus-4-0-20250514, claude-sonnet-4-20250514
# OpenAI: gpt-4o, gpt-4o-mini, o1-preview
# Gemini: gemini-2.0-flash, gemini-1.5-pro
# Ollama: llama3.1:70b, llama3.1:8b, codellama:34b
model = "claude-opus-4-0-20250514"

# Enable thinking/reasoning mode (recommended for PRD)
# Provides deeper analysis but slower and more expensive
thinking_enabled = true

# Maximum tokens for response
# Range: 256-32768, Default: 4096
max_tokens = 8192

# Temperature for generation (creativity vs determinism)
# Range: 0.0-2.0, Default: 0.7
# Lower = more deterministic, Higher = more creative
temperature = 0.7

# =============================================================================
# Architect Agent Configuration
# =============================================================================
# Creates system design and task breakdown from PRD
# Recommended: Use thinking-enabled model for complex design decisions
[agents.architect]
provider = "anthropic"
model = "claude-opus-4-0-20250514"
thinking_enabled = true
max_tokens = 8192
temperature = 0.7

# =============================================================================
# Ralph Loop Agent Configuration
# =============================================================================
# Iteratively implements tasks until completion
# Recommended: Use fast model for many iterations
[agents.ralph]
provider = "anthropic"
model = "claude-sonnet-4-20250514"
thinking_enabled = false
max_tokens = 4096
temperature = 0.7

# =============================================================================
# Telemetry Configuration
# =============================================================================
[telemetry]
# Enable/disable telemetry entirely
enabled = true

# Service name for telemetry identification
service_name = "ralph"

# Enable distributed tracing (spans for each operation)
enable_tracing = true

# Enable metrics collection (counters, histograms, gauges)
enable_metrics = true

# Log level: trace, debug, info, warn, error
log_level = "info"

# OTLP endpoint for exporting telemetry data
# Leave commented to disable OTLP export (logs to console only)
# otlp_endpoint = "http://localhost:4317"

# =============================================================================
# Example Configurations
# =============================================================================

# --- Cost-Optimized Configuration ---
# Use Sonnet for all agents (faster, cheaper)
#
# [agents.prd]
# provider = "anthropic"
# model = "claude-sonnet-4-20250514"
# thinking_enabled = false
#
# [agents.architect]
# provider = "anthropic"
# model = "claude-sonnet-4-20250514"
# thinking_enabled = false
#
# [agents.ralph]
# provider = "anthropic"
# model = "claude-sonnet-4-20250514"
# thinking_enabled = false

# --- OpenAI Configuration ---
#
# [agents.prd]
# provider = "openai"
# model = "gpt-4o"
# thinking_enabled = false
#
# [agents.architect]
# provider = "openai"
# model = "gpt-4o"
# thinking_enabled = false
#
# [agents.ralph]
# provider = "openai"
# model = "gpt-4o-mini"
# thinking_enabled = false

# --- Local Ollama Configuration ---
# Run models locally without API costs
#
# [agents.prd]
# provider = "ollama"
# model = "llama3.1:70b"
# thinking_enabled = false
#
# [agents.architect]
# provider = "ollama"
# model = "llama3.1:70b"
# thinking_enabled = false
#
# [agents.ralph]
# provider = "ollama"
# model = "llama3.1:8b"
# thinking_enabled = false

# --- High-Quality Configuration ---
# Use Opus for all agents (best quality, highest cost)
#
# [agents.prd]
# provider = "anthropic"
# model = "claude-opus-4-0-20250514"
# thinking_enabled = true
# max_tokens = 16384
#
# [agents.architect]
# provider = "anthropic"
# model = "claude-opus-4-0-20250514"
# thinking_enabled = true
# max_tokens = 16384
#
# [agents.ralph]
# provider = "anthropic"
# model = "claude-opus-4-0-20250514"
# thinking_enabled = true
# max_tokens = 8192
